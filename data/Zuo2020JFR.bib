@article{Zuo2020JFR,
author = {Zuo, Xingxing and Ye, Wenlong and Yang, Yulin and Zheng, Renjie and Vidal-Calleja, Teresa and Huang, Guoquan and Liu, Yong},
title = {Multimodal localization: Stereo over LiDAR map},
journal = {Journal of Field Robotics},
volume = {37},
number = {6},
pages = {1003-1026},
keywords = {computer vision, localization, perception, position estimation, sensors},
doi = {https://doi.org/10.1002/rob.21936},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21936},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21936},
abstract = {Abstract In this paper, we present a real-time high-precision visual localization system for an autonomous vehicle which employs only low-cost stereo cameras to localize the vehicle with a priori map built using a more expensive 3D LiDAR sensor. To this end, we construct two different visual maps: a sparse feature visual map for visual odometry (VO) based motion tracking, and a semidense visual map for registration with the prior LiDAR map. To register two point clouds sourced from different modalities (i.e., cameras and LiDAR), we leverage probabilistic weighted normal distributions transformation (ProW-NDT), by particularly taking into account the uncertainty of source point clouds. The registration results are then fused via pose graph optimization to correct the VO drift. Moreover, surfels extracted from the prior LiDAR map are used to refine the sparse 3D visual features that will further improve VO-based motion estimation. The proposed system has been tested extensively in both simulated and real-world experiments, showing that robust, high-precision, real-time localization can be achieved.},
year = {2020}
}

